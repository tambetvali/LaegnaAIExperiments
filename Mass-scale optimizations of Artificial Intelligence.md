# Mass-scale optimizations of Artificial Intelligence

## Types of providers

- Individuals do personal training, **fine-tuning** and **providing documentation** to an AI.

- Large companies do cloud-based **training**, fine-tuning and **collecting documentation** and **processing** it.

- Small companies or rich individuals do somewhat organized process, which might qualify for more than individual training in your laptop or PC: they might have access to more resources or provide small collectives with cloud time, hardware needs and sources for documentation.

- Open source collectives and initiatives can collect more individuals, who each provide some amount of processing, such as partial fine-tuning, loose standards or, in opposite, divisions of specialization; collectively, they can also buy cloud time, for example if each pays for their own (for example, providing 10 cards means you pay for fine-tuning on 10 cards and related work to get it into a cue, or you pay for example for 200 iterations to let do various things with your cards or support others as well, and gain access to their valuable data). Such initiatives can also look for large-scale sponsors, who support major operations with their flashcards and desires: for governmental initiative, this might come from taxes.

We can imagine many different providers, who form individuals with their own fit or unfit desires, such as researching a niche area, and who form collectives and clever forms of organizations; they might have conflicts, protect their data from each others, or differ in their understanding of truth, how to tell it, when to not tell it etc.

## Types of models

We need to ask a list of questions to meet the needs for particular models:

- Low-latency, exponent models of various levels are models, whose minimal activity takes all the resources at such level.

- Modular, linear models are models, which have modules, which can be trained separately, and which do not thus form exponent qualities as a whole.

- High-latency, loosely connected infrastructures of an AI might not need unitary training resources.

Terms:

- Low-latency: this means a training resource must have low latency, it's pieces work closely together or form a cloud.

- High-latency: this means a training resource can have high latency, it might be loosely connected and slow network connecting the pieces of virtual machine used for training.

- Exponent: it uses up all the resources without providing reasonable length of an array, but being able to train less such items at once.

- Linear: it can use the same resource in multiple steps or phases, and it's a linear resource like CPU, not exponent resource such as GPU or cloud, which work on parallel and massive scale.

I did not turn the terms into opposites - the term of the model is exactly what it needs from a network, to understand how they qualify together (this kind of non-turnedness is actually an old principle of hermetics and alchemy, which removes certain artificial complexity and is considered one of it's 7 axioms: "as above, so below" means that "above" is the positive quality we get from the model, and "below" is the cost: rather than building complex formulaes and quadratic calculation, we look them in better way than linear - they are just the same variable; still you might need to measure certain aspects as if they were opposites, in certain terms).

## Types of virtual machines in terms of their latency, linearity and ability to solve exponents

We are interested in several qualities, which tell us whether they can solve exponents in our resource needs, which ones etc.

### What is exponent

**Logarithmic** growth is growth, which slows down as the function grows, and **exponential** growth is growth, which speeds up. Linear growth does not change it's speed and thus forms a line and rather evenly distributed density of it's points. Logarithm or exponent factor measure the same thing, and one is opposite of another: while the basic definition is met, the factors can be different.

Velocity is linear, acceleration is exponent and decceleration is logarithm or imponent (sometimes, my preferred word for clarity; while latin has the prefix "im-", it's used rarely).

Same function can have layers in logarithmic and exponential direction, differential and integral functions divide them into "wavelengths" in this matter - coming in steps from functions nth integral to nth dimension creates kind of fourier diagram of such wavelengths, and it has also modular arithmetics where you measure them as distinct components in distinct dimensional scale.

## How it measures

We can see plus and minus, with multiplication and division, are operations where order matters: on different scales and levels, exponential/logarithmic and linear growth factors form complex web of parameters. For example, a \* 2 + 4 is different from a + 4 \* 2, and thus it matters a lot where we form those factors.

* Forming the factor *above* AI functioning means it's atomic compounds can be linear; from this linear subshell or subsphere, to certain scale, what follows is exponent of AI functioning; multiple requests can be made in a row or in parallel, with the same model repeated - this repetition is also linear; number of layers and self-attention layers is linear as well.

* We find more interesting factors, for example setting number of layers, atom size and length of series of queries into large numbers, this could suddenly behave like exponent. Measuring this linear layers, each has multiplier "1" for it's base size - 1 for atom might be size of floating point number, and 1 for layer is 1 layer, and 1 for simultaneous queries is 1 query.

We can see how linear and exponent scales become somewhat relative, but critical to our speed measure.

We generally say:

* It's rather as more linear as longer arrays we can create, in dimensions which do not impose their interdependency.

* It's rather more exponential if we manipulate small numbers of these units, and are unable to scale.

We can see that 2 + 2 + 2 is linear factor, but 2 \* 3 is exponent factor, as long as the numbers are of free variables, but the count of those variables and operations in between give them constant shape - as soon as adding one "+ 2" can be done simultaneously, indeed 2 + 2 + 2 would rather equal 2 \* 3; unless we can do this, they have spatial difference in shape of this operation. If we measure 2 + 2 + 2 with logarithmic unit, such as counting how many hours we would work without break, we would need single, constant value to not even go further, or an exponent which could overplay that imponent.

We can generally easily measure exponent and linear factors, but we should consider this reasoning.

## Model factors

In larger architecture or flow of operations, we can modularize the process:

* We can create subconscious awareness such as dreams of symbol language, which is not expressed by the model, but either in GPT-like, layer-by-layer fashion, or rather static fashion which produces output from input, it can create a variable space, which can *simplify* the process. This way, you give a calculator for the model, which it uses subconsciously: the optimization process is roughly the same, but for example you might work on each 5 bytes and recognize all names which are that long; this linear operation can be attached in various ways to hidden layers.

* We can create classic calculators and combinators, which give special properties to areas inside the neural net.

* In this way, it's not fully a self-learning system, but has some *manual organization* or modulus; most human tissues work that way, and it's considerable as an instinct for the end model.

Modularizing the process, we linearize aspects of model in terms of space (requirements of memory, compute cells, storage and bandwidth) and we give chance to people to fine-tune their basic parts.

Generally, we can do all this to linearize:

* We make models be used collectively.

* We have model specialization, where it uses all it's consciousness for a definite subset of tasks; and rather than GPT-like *time-ordered* reasoning of multiple layers to form a word or sentence, simpler information lets us still use Deep Learning, but get rather instant answers.

* We use linear tools: imperative and logical languages, some provers or fact-seekers, dictionaries and database engines in between, either from inside model, from it's toolset or how the model is combined into our process.

* We use advanced algorithms, which have high latency: they allow for calculations in high-latency networks.

## Hardware factors

We can see fractals, holograms and other complex forms appearing in this organization of hardware.

Those things appear in scales, where similar units are used in parallel or in chain, or each unit is different; sometimes the difference is not intentional, but for example different users can set same goals, but buy different computers and chips; we only use this chaotic aspect, unless we need to write drivers and use special apis - normally, our needs can be distributed as chaotically:

* Logic unit scale, which contains very trivial units.

* Operation scale, which contains trivial, yet very complex units.

* Parallelization scale such as GPU operation parallelization.

* Cache levels are direct scales of organization.

* Pipelines which remove some orderness of time, where operation is moving like in factory system, and first workers of the cue do an operation past in time, compared to last worker who hands over the result.

* Cores, multiprocessors, unity of GPU and CPU etc.

* Systems: computers, server blocks, designs connected with short and fast cables.

* Clouds, societies who own hardware, collectives etc. follow similar fractal - homes are very small uniting a few computers and phones, companies are larger but often tight together, nations or governments might be rather loose, the civilization on earth and the planets can have *poor* connections, such as connection with NASA's devices on Moon or Mars, which both have lag worth minutes.

* Virtual machines, virtual networks, and information space, compability and other kind of architectures, which connect all this resource, make it more complex.

### Factors inside this fractal, field or hologram of devices and information-operation properties

We have the following qualities, which need to fit our model or architecture of models and their use or fine-tuning:

* Basic operations are optimized and work well on all basic units; this theory has very good and available research for decades, and many new specifics for AI systems, such as need for tensor units and special attention to GPU "side-effects" (as it was initially seen).

* On each level, we have distance between computing units as low-latency unit or barrier, and cable or connection quality which can be high-latency unit if it's fast. By this, we need to separate parts of our model process; for example, each unit in a cell network cannot randomly depend on others, but rather it uses nearest neighbourhood or otherwise special, hardwired connections between specific units; for example, assigning each cell another ID but only if it's active means we cannot resolve this in time, while each unit could calculate an ID based on it's position if the network is highly coherent and symmetric in structure, with self-replicating elements.

* In each device on each level, we have speed factor and performance of it's operations: as better it performs given operations, as faster we can see it. We can use specialized devices and assign model parts or process parts special needs in typical setting we see - for example, running mostly on balanced CPU+GPU machine, we would also want to see this balanced CPU+GPU shape in our model or system: it does linear and parallel operations in balanced way; this is how it rather does not oppose, but reflects it's physics by it's psychology and function.

* We have storage points, and their size matters; each unit can get storage in some time, and needs some time to give it over to initial mechanism. Thus, it has mutation latency: each time a machine or part is changing it's behaviour and especially the data schema, it needs some time; so our model function must "need" this time as well for these functions, to talk same-way.
  
  * Side-effect free storage: we have critical points where things are calculated; after this, the information and calculation shape can have series of operations, which can be reproduced; cheap cost of this reproduction decides that this information is side-effect free, and we can have less storage operations on it.

So, we have logic units, chips, people and organizations, which are:

* Either close or far apart in terms of cost.
  
  * Synchronous model has modulus for this - different units must have different modules for optimized model to run, reflecting the shape of it's container.

* Form dense clusters, which are apart: normally, for example, each computer has a scale of computation device, does many operations or contains many logic units, which is dense point; computers themselves are apart:
  
  * Latency of network decides our model is modular, but also has less dependencies in time: it can wait.
  
  * Chips, users and organizations do not download and upload large databases all the time - for example, to train your modular unit, you download only a subset of the whole model knowledge base.
  
  * Here, we have to design the model information flow - it mimicks the actual situation to take advantage, and does not see this as disadvantage by having different comfort zone.
  
  * This kind of clusterized distance also forms, when we use certain times of cloud, or for example do training and fine-tuning at nights, when our server has less customers: each time we redownload the data, even if from our hard drive, to be cached and used with low latency, and then we upload it: this process can go though version control registration and other processes, which *might* have additional latency in terms of our model work.

* They form same types of hardware, API's, desires and possibilities, or different. If this is variable, our model reflects environment shape by being variable-sized: for example, it can have array of units, but it simply accounts for it when it changes from 100 active units to 200 and back; very often, in clouds, we wait for other clients.

## Shape of information

Humans, as well, form efficiency structures: in this common thermodynamic reality.

We also need time to hand over the information, we have specifications or unknowns and bugs in which information we can handle, we are close together if in collective, or far apart if this is a "free wall" or community connected by an idea, rather than a factor or "reality".

Our knowledge base needs to be organized in way that people download some original data, work on it, add their own, and upload it: within normal human limits, we can see how linear and exponential qualities, as well as lags and efficiencies or disefficiencies of processing form - our model and especially it's intent must be coherent and form similar shapes, for example it excepts to hand-over some data in resonant way to humans who made it, for example it's pieces of quality information or hand-made summaries form patterns, which are dependent of original information.



# Training

Have your program manual, along with many other manuals in roughly similar language or scope, or which provide contrasts and patterns: this is 33% of random fine-tuning samples.

Have some set of general data, which keeps tension with global sphere, not letting your optimizer take too unknown dimensions or lose tensions on it's learnt patterns. Keeping cards together is weak inference gain: the card data reacts in various ways, if it's given close in session, and the context is vibrated around; this is an effect which must appear in any free optimization of parameters.

If you can create some examples, initially or based on your program now after finetuning, with fine-tuned model, work to get to examples: 1 example provides first exponent in understanding, next 20 provide second exponent, next few thousand a third one and then few hunderd thousands start finding local patterns; keep them distributed with other domain-specific and general areas to not get too habituous or focused, still keep the number of your cards high as this effect is somewhat general as it seeks a resonance.

## RAG

Write your program's usage: what AI will ask, what will be answered, guide through program application and how AI plays the service provider api itself.

Write your ptogram internal specifications, several such cards can be worked on in order, or Prolog-type of connections allow multiple runs over linear added complexity, where reading all cards once is exponential complexity.

In both fine-tuning and RAG, usage of the model, the "Character" is where an AI knows they are exactly provider of this service; train mixing with content not from providers of this service - the patterns, which inference in training, must be grand.

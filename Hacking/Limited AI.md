# Limited AI

Today, normal users have many limitations in regards of AI - locally, they can only run small models, and they do so slow.

This is only a limitation, but it's not limited: we do not see so many proud hackers, who did their "this" with that kind of limited AI.

Often, we like to limit our art to ASCII, create a web server from only one printf command, show how we ran a decent thing on smallest microchip ever. Many minimalist activities identify hackers: people, who want to hack it together, to do it without many provisions, with slowly a dedication. People, who want to do it at home.

Such peaceful hackers, classically, are people who bring a broken table to their home and fix it, still work with that 10 years old car: hacking is a manly tradition of ages past.

## How to limit an AI habit?

If we think of habits, rather than intellect, we get to know what a small AI is:

- Rather than running through every moment thoughtfully, their training sessions are already very much that: they are supposed to follow closely what they did at training and fine-tuning, with a small number of free variables and variable things.

- If we think of habits, we can sometimes occur into funny situations: we do this habitually, but the thought of the situation will be surprising us.

AI behaves very much like humans in this sense: for especially it's limitations, and often for it's creative surprises, we can measure that it's limits and abilities share many common syndromes and synonyms with humans, even if we should carefully rephrase it - saying it with robot tone, sometimes, could be still enough rephrasing, where certain details might differ.

Limitations of attention, wisdom and concentration in humans are very often fixed by traditional means - workers are taught strict regulations, brought back if they lose their trackful sources, and the language is simplified to trivia and "practical application".

## Limited AI - a practical man

We achieve limited AI be these way:

* Programming: rather than conscious thought of an AI, we measure and stipulate careful agreements, "programs" it should follow. For example, a work environment with it's rules and specifications is a "program" - we have programs for guiding such environments, as well as programs for free time activities. Especially the dangerous things, are brought by by strict regulations and strictly ordered fashion.

* Training and fine-tuning shape the abilities of our AI, where the "Q" or "Question" contains question, and tagged chat history, toolkit descriptions and manuals, character etc. - "Q" is rather a syndrome of all the input. "A" will contain toolkit commands, answer, modifications to variables, files and states - all the "Output". In generalizing an AI, if we write a class, inputs of every variable are somehow in "Q", and outputs in "A" (it's rather more complex, but clearly, the technical Q&A form might not be the specification we think in; every "A" becomes part of "Q" in time, as much as it's matter of this linear progress). Now, in terms of this training and fine-tuning exercise, we need an AI to not spend much time on "games and amusements", information we could feed for contextual habit: rather, we want practical information to pass through. We want practical information to pass through the "Q" and "A", where AI loses *focus* and uses *attention* to lose their soul in *drinking* - we rather want an AI to *eat* well. We want clean and tidy working environment, where unrelated sensory stimulation occurs only to get it vaccinated.

* Commonly understood, workspace language and terms: in any specific field of a specialist, we clearly define terms. A small AI can be smart in science questions and answers in limited domain with specific, clearly defined language - indeed, it can be a good companion if we understand our terms. With tools like SpaCy, we can turn more general language into more strict subset, converting our input to an AI in context-free manner, and we can turn it's strict answers, again, into freeer language use.

* Common worker is limited to normal, everyday situations, activities, occurrings and even outcomes: once the situation gets out of track, engineers and bosses come to do. We generally understand what it means to "be on track" for humans: so it's close for an AI. Simpler people, generally, seem to build their space on less dence network of cells, or more insufficient cell communication - complexities of atomic variables. It's hard to say what it means to be "practical" and "common", but clearly it means a bright thought is avoided internally and often externally, and the kind of "practicality" is fought in determination - this small model really *needs* to be that kind of practical.

Now we cannot say that small AI does not have mind at all: we *can* have some free variables, small in size, fitting into it's network of decisions. For a human as well, practicality does not mean we won't think at all - rather, we think in order, in small pieces, and with a lot of consideration.

## Interfacing

To train large AI, which can work with small AI subsystems, we need to train it on side-effect free blocks.

Imagine a list:

List summary (item 1):

* Element 2

* Element 3

* Element 4

We can create a process to work through the list:

1. Process each element aligned with summary: 1+2, 1+3, 1+4. It might be associated with other list, where they are processed in pairs.

2. Process the whole list at once, but summary length is 1/4.

Each element has longer body or spec, where this "summary" is api - interface we use for programming our application, such as practical need. Programming means we work on this list with predefined and determined order, always the same; application means we use it for something practical; and interface means it's not the whole thing, but a bottleneck.

Large AI can:

* Work on the whole list at once: it will do this, but it makes sure it *still* creates specific rules; for example it might built two links into resulting list elements, limiting the connection to 2, not each element with each element.

* It must make sure each element or summary does not rely on side-effects of others or produce ones outside the *interfaced* process, where we strictly follow the web of bottlenecks rather than magically effecting the content on the outcomes.

* Thus, it's work must pass kind of "blind tests".

These 3 regulations are rather strict: while the large AI connects elements and finds flaws in the general picture, it must explain carefully. It has generic knowledge, but also an ability to limit it's language and scope.

## Freedom

While the question is side-effect-independent related to broader context, while it might rely on *named and taught context*, and it follows premade patterns, it is still:

* Intelligent machine in it's small problem scope.

* Able to learn certain amount of creativity and localization, where the patterns are not completely strict as in formal programming.

* We do program it not completely with programs, but with same kind of limited-scope, interfaced orders as well: looking at headers/interfaces/APIs of given set of conditionals, constraints and matters, it takes a task and builds a program: an instruction, where tasks come in order (such as task 1, task 2, task 3), their outcomes matter and can be used to formulate conditionals - for example, fulfill task 3 if task 2 is successful, or jump to task 4 if not.

By this kind of freedom we can see we can define a *loose programming language*, where human understandability rather helps AI understandability - we together have different limitations than strict computing set.

## Working environment

Imagine a master craftsman guiding ten junior workers: each is young, and supposed to get into certain ability to resolve the common cases. Senior craftsman, indeed, has seen a lot and is able to identify and cope with exceptions: but their time is very valuable.

Now imagine this set of AI models:

- You have one large model: either it works at nights, spending two hours on an item, or you buy it from a cloud or simply use a service which is not made by you; perhaps you work offline and gain access to this only when internet is available.

- Imagine you have ten small models designed, trained and fine-tuned for specific kinds of tasks, with fully supported RAG for this task. While this is very patternized, there is still some flexibility and joke, a healthy amount of it.

While the senior craftsman has more general guide and is quite free, they still must know the specific limitations and abilities, as well as language and math use of those juniors - it's very much like a master, mastering good leadership skills.

The juniors have been trained for this specific work habit - imagine if they have free time, they replace the model kind of; one model is not only one character, but rather a character in specific mode. For juniors, you have "programs": you have set strict guidelines, specifications and regulations.

## What you gonna do with a junior worker

You have this specific thing:

- They are not completely specialized for *one task*, but a pattern of work which is about half of complexity they can account for; the exceptions and side-effects of writing anything at all, where invisible patterns appear on their own, and our absolute need for some freedom to overcome limitations to *know it all* strictly: so we keep the main headline going in our stream, but this accounted complexity is about half of the AI's, while we break certain cases of our language and flexibility helps us in what we cannot foresee (so it's somewhat *agile*, in terms of programming).

- We build up more or less formal language, but rather than building the one which goes only one-directionally, not seeking for the future, we follow *human-like formalization rules*: it's not a strict programming language, but formalized language which allows for programs, specifications etc.

- If background workers are very specific, one waitress-type of foreground worker can be more free - in kitchen, the cooks can work very strictly, but the person who talks with you might understand your joke and "personal character", which are not their strict regulations.

- We understand that "final specification" is kind of impossibility, a manner of IBM-scale work which still seems somewhat boring or boxed. AI is good in exactly this: while *local scoping* needs language which is not very flexible based on the context, the token window size determinates that with short speech, we must be already quite specific and clear, and the count of layers means that inside this small scope, we can still be quite intelligent: inside their small scope, workers solve many cases intelligently.

## Mimic octopus

Side-effect freedom: general context, past history of task etc. must not have side-effects on process, but all the effects must flow, in programming, inside the small bottleneck of each task; the answer must rely, solely, on the short question, and the context it feeds-back to the question is also not dependent on the future; "side-effects" are all the influences between things outside of this bottleneck. The bottleneck can move in various ways, for example logic constraint language might set up order of combinations to be done, but more typical imperative program sets up steps, where each step uses the limited Q and A windows, and divides them either to array of short pieces of text, small variables, or one larger; you do memory management to get all the tool's instruction, work and output.

You build mimick to your tasks on similar patterns, or which can be clearly identified as separate tasks - there is a number of this. You describe the characters of an AI, give it questions and answers based on the character, and change the characters. It will learn patterns, but they remain inside the context window, and build some layers of subconscious process, which can explain the task *only based on this window and "globals", universally defined terms*; "globals" can be defined also to fit exactly this local context, for example if it's said to be woodcutting task, well-known or characteristic to this model, facts of woodcutting hold, but the specific locals must move in predeterminated manner.

The interpreter is not reading code from beginning to end - it is still a "flow".

Past context might disappear in long process.

Now learn this:

* Set up the "global variables": these are background facts, which remain always the same and thus, are not inside your work context. Imagine there is still some limit of memory - the pattern itself has limit of complexity, determined by the hidden layers which provide additional relations with given, local context, and are determined by intelligence - somewhat, the "intelligence" of such manner determines the capability of the memory. As more patterns those details have, as more can be memorized, and as more unique, specific and creative - as more you must make sure there are less of them, and especially you are using less of them in the same Q&A pair, in the same local context.

* Invisibly, if the token window contains 10 Q&A of your given size, each Q counterpart in Q list [Q=q1, q2, q3..] and each A counterpart in A list [A=a1, a2, a3]; in this programming-kind of environment and not free speech, we could call Q&A also I&O, input and output: this is useful if we compare this with memory management. Altough this is linear list in Q&A, all those items are still considered each against and side of each other - this AI model is not capable of linearization; where we find fully linear traits, is *distinct tasks of simultaneous order*.
  
  * We are not rigid, but leave flex and thus use less of AI power than it has - exceptions and failures occur randomly and not above some threshold; but we really imagine commands and variables in this process.

How we write this task, must mimick this incapable AI in various tasks and processes: it can generalize more if there are more tasks, so this is not a problem, but the context size grows noticeably if the patterns are very different; this is imaginary kind of memory which stores *patterns* and pays penalties for *exceptions*, not a real size of it's knowledge base, such as weights and biases: we must consider with humanlike intuition, what it means to have more patterns, and cannot measure this in terms of computer memory directly.

### Miming

Miming means this:

\<characterQ>User, you are a math student who has passed limits, but does not understand matrix multiplication.\</characterQ>

\<characterA>AI, you are dedicated and patient math teacher who knows many examples.\<characterA>

Based on those two characters - normally, you got AI character and the "topic", but here I want to show general information -, you need to mime question and answer.



The other one:

You teach an AI a toolchain: the toolchain must be consistent mimick of a file system, webpage access with cached website you desire etc.; the session IDs need to be unique and have consistent pointers accross the "session", which is mimicked.

This means you have to simulate the questions and answers, make sure the questions to toolchain are meaningfully connected to *information which is already present*, and that you connect the answers meaningfully: so you do not have your own background knowledge, but you do what you do based on the initial question, and your system needs to *mimick* the use of toolchains for different cases.

For example, toolchain answer can be "Q", while the command itself is an "A", so that your AI has conversation with toolchain and you: with each "Q", you include the local history with special tag in beginning of "Q", and before this there is also tagged information about available tools, their basic manuals or instructions, and commands to get more help to use them. You need to work through memory management: either your AI is trained to use this toolchain, and wins from short docs and instructions appearing, or it's not trained and you must be very careful with docs; or if this toolchain is native to the AI, probably just the name and version of that tool is sufficient. The answers given to tools appear in context memory of questions in next part of the dialogue: so you create one Q&A pair, which contains all the previous pairs and updates to the tooling (context might change), for each part where the flow of information - input or output - has been changed twice. Indeed AI might have different types of toolchain access, but this is the example way which should definitely work with all the models, where AI interaction is directed to either a tool, or to you. If your AI consists of multiple interfaces and modes, separate solver of tasks and interactor with you, etc. - indeed the models might be very different, but the important point is that you need to *mimick a situation* to teach it.

## Side-effects journalling

We must make sure this:

- Set up our common background, and context inversion: we make sure a set of facts is *common*, and the local change into this variation is *known*. The background system does not have side effects means that: we can clearly define how we find each case of context from model description, and the local Q&A content, without relying on surrounding process which is entered automatically, without AI-given thought at "*current moment*".

- In one Q&A pair, there is no linear order, like in iterative programming - this linear order exists in environment -, but there is *order of dependencies*: we must make sure, for every fact, we can understand what it depends on: it's either an input, depending on past process clearly, or it's a transformation, which must depend on input. The output, also, is determinant: the AI flow is consistent with it's output, so we can look at this lack of side-effects also in reverse, making sure a given output is achieved, based on produced variable which clearly depends on it - flow of an AI makes it somewhat "oraculous" in this sense, that it's consistent to it's future flow, taking hidden directions as a "side-effect".
  
  - We must mark exact parts of input and decided output, and make sure decisions or information does not appear out of nowhere: or simply, we must clearly find out that each fact in the result is *solely* based on input. When we are training an AI, we are looking it at a scope, but we must find a symmetry for determinism, which I call a "mimick octopus".

- In program flow, the side-effect freedom means what it means in normal program: output depends on input.

What I describe, might look much like classical programming: but make sure you also know, even this small model is intelligent and your performance depends on using this intelligence.

For example, what I describe looks very much like *imperative programming*, but you must also understand that in regards to an AI, this is *emulative principle*: as long as your memory model is consistent, you do not have to reprogram it to make it part of logic programming system or theorem prover: as I said, you can very much imply things from future to backwards, because once the AI starts the flow, it's already conditioned by the future by backgradients and repeated training.

Rather, take it so:

* It's intelligent model, but *whole it's universe is determined by the context window at every given moment*.

* You work with input and output cards.

* Programming a small AI means you have to learn *stateless models of interaction*.

* Programming large AI differs: with enough context, it *seems* stateful, and with perfect amount of context, it indeed *is*.

Stateless and stateful: stateless program, at moment of it's working, is not interacting with anything outside, but rather where the Q&A itself is the *state*, the condition of an AI is not changing, in reliable or meaningful ways, based on what Q&A it got before: rather, in beginning of the question, we resend the conversation history each time, which is a normal way to emulate states in a stateless machine. A stateful program would take questions, give answers, and become smarter out of the way it's built.

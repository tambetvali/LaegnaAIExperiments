# AI models

**Most of the text is generated by CoPilot, but I mention it excplicitly in case I refer to it's explicit notice. Btw. him/her/it? How it is doing?**

Finding correct, working AI models has been work; the tools, libraries and models mentioned here: in many cases, selecting a few from general class of tools or opportunities has been part-time work of 2-4 weeks, with several disappointments and finding strategies out of limitations; for this, I think it's important to bring a small list of AI models, which at least work.

# CodeLlama

You can get them to work on Ollama, which is a well-supported server, and there are various plugins and tools they can cooperate with. I got them downloaded and installed; on VSCode extension, they were extemely slow and broke on it's new version on my 15GB memory laptop: altough, directly inside codellama they are relatively fast on the same computer.

Use them for coding, they might not want to talk about anything else and probably change the topic or try to code it.

### CodeLlama 7B
- 7B parameter model  
- Fastest and lightest Code Llama variant  
- Good for local use, code completion, and smallâ€‘context tasks  
- Available in Base, Instruct, and Python versions

### CodeLlama 13B
- 13B parameter model  
- Stronger reasoning and codeâ€‘generation quality than 7B  
- Better for complex coding tasks and longer contexts  
- Also available in Base, Instruct, and Python variants

# 1-3B models

CoPilot: **Small models (0.5Bâ€“3B) are now a serious category**, optimized for onâ€‘device use, fast inference, and lowâ€‘cost deployment. The most prominent families today include **Qwen2.5** and **Llama 3.2**, both of which explicitly ship 0.5Bâ€“3B variants.

Qwen: I have verified it to work from 0.5B to 3.0B, not giving me absolute nonsense. Several other advertised small models just became trapped to nonsense loops, but might not be broken: probably, they need configuration and adaption with their limitations; anyway, we are speaking of things, which work straight forward.

Small models do not make sense if you use large servers, like GPT4All and LM Studio: rather, running them in Jan, lightweight client, which currently does not support document collections, gets answers to your small, relatively context-free questions quickly, so that you can win from this benefit of being "small" (for example, asking before you annoy the bigger models).

Phi models, noticeably, give me nonsense and do not work in Jan, GPT4All etc.: they might expect some configuration, or have different standard of templates. I do not know whether Microsoft wants to fix this, and they might help you somehow in their products: reporedly good models.

I have some good experience with Gemma in this category.

If you want to fine-tune with weak computer, this class of models gives you fast way to test this - summarize your cards to half in length and try out whether your general fine-tuning strategy would work in long term.

# Fine-tuning

Copilot: **LitGPT supports more than 20 model families, and many of them can be directâ€‘downloaded and fineâ€‘tuned depending on your GPU memory. Below is a clear, structured breakdown based on the latest information from the LitGPT GitHub repository.**

# ðŸ§  Fineâ€‘Tuning Requirements by GPU Memory

This is CoPilot's list of what you could automatically download and train with LitGPT. I did make the models I somewhat trust (to respond and be good for your experiment, not to be very intelligent in each case) into bold, but notice I have not tested them all, so there is no such thing as non-bold here.

## A. Very Small GPUs (4â€“5GB VRAM)
- Use QLoRA, FP4, or 8â€‘bit training.
- Best models:
  - **Qwen2.5â€‘0.5B**  
  - **Qwen2.5â€‘1.5B**  
  - **Llama 3.2â€‘1B**  
  - **TinyLlamaâ€‘1.1B**  
  - Phiâ€‘2 (with aggressive quantization)
- Limitations:
  - Models â‰¥3B require CPU offloading.
  - Models â‰¥7B are not feasible.

## B. Small GPUs (~15GB VRAM)
- Ideal for 7Bâ€‘class models using QLoRA.
- Best models:
  - **Llama 3.1â€‘8B**  
  - **Llama 2â€‘7B**  
  - Mistral 7B  
  - **Qwen2.5â€‘3B**  
  - Phiâ€‘3â€‘Mini (3.8B)
- Capabilities:
  - QLoRA on 7Bâ€“8B  
  - Fullâ€‘precision on 3B  
  - Multiâ€‘epoch training on medium datasets

## C. Large Systems (40â€“80GB+)
- Suitable for large dense models and MoE architectures.
- Best models:
  - **Llama 3.1â€‘70B**  
  - Mixtral 8Ã—7B  
  - Mixtral 8Ã—22B  
  - **Qwen2.5â€‘72B**  
  - DBRX
- Capabilities:
  - Fullâ€‘precision training  
  - Multiâ€‘GPU FSDP  
  - Pretraining  
  - Largeâ€‘batch RLHF  
  - MoE routing optimization

# Around 8B

Qwen works, and Gemma also starts from 0.5B and works.

# Larger models

Those require heavy hardware and you have to try yourself for truly intelligent models:
- This list actually gives you what you need to do some fine-tuning sessions with your cheap computer, for example to convince a model to properly be your tool in simple tool using AI and to gain some efficiency.

LitGPT fine-tuning for these models:
- It's possible you don't need very small model, but well it might be a good idea to start from small model you can still use: you can try less context-sensitive things and understand in days, whether your planned product with weeks of actual fine-tuning is worth tuning and whether the plan is now good; I think with larger models it can mostly only be better.
- LitGPT also has some automatic sources or servers for standard fine-tuning data: you can use this either to do a fine-tuning test session, to see the speed and to understand that your client side works:
  - Then create a simple service to provide JSON in way that LitGPT can be fine-tuned.
  - Train on this, then train on standard data; repeat this: probably, you can make your own service a proxy to this kind of general service in way that you redirect it's cards, and randomly add yours.
